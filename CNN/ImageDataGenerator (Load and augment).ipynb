{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ImageDataGenerator (Load and augment).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgleGwk6P3wmigZNnCkrko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"G5UwZR-qKGoq","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOk3q9z9KWZP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"ok","timestamp":1592546841747,"user_tz":-480,"elapsed":4557,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"b2a9f0af-0ac8-44f4-e4cb-932f475efb61"},"source":["#Download the files for training\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n","    -O /content/horse-or-human.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-19 06:07:19--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 2607:f8b0:4001:c18::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘/content/horse-or-human.zip’\n","\n","/content/horse-or-h 100%[===================>] 142.65M   130MB/s    in 1.1s    \n","\n","2020-06-19 06:07:21 (130 MB/s) - ‘/content/horse-or-human.zip’ saved [149574867/149574867]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oADE4z4zLqOr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"ok","timestamp":1592546843392,"user_tz":-480,"elapsed":6183,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"7568994e-ef3f-4b30-d5db-97c9f2c209b9"},"source":["#Download files for validation\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n","    -O /content/validation-horse-or-human.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-19 06:07:22--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 2607:f8b0:4001:c0e::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11480187 (11M) [application/zip]\n","Saving to: ‘/content/validation-horse-or-human.zip’\n","\n","\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/validation 100%[===================>]  10.95M  --.-KB/s    in 0.1s    \n","\n","2020-06-19 06:07:22 (78.8 MB/s) - ‘/content/validation-horse-or-human.zip’ saved [11480187/11480187]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"InDU4A3hOR7l","colab_type":"code","colab":{}},"source":["import os\n","import zipfile\n","\n","#Unzip the training and validation test sets\n","local_zip = '/content/horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/content/horse-or-human')\n","local_zip = '/content/validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/content/validation-horse-or-human')\n","zip_ref.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PVw46o5Or4x","colab_type":"code","colab":{}},"source":["# Directory with our training horse pictures\n","train_horse_dir = os.path.join('/content/horse-or-human/horses')\n","\n","# Directory with our training human pictures\n","train_human_dir = os.path.join('/content/horse-or-human/humans')\n","\n","# Directory with our training horse pictures\n","validation_horse_dir = os.path.join('/content/validation-horse-or-human/horses')\n","\n","# Directory with our training human pictures\n","validation_human_dir = os.path.join('/content/validation-horse-or-human/humans')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDmXaCpRO_d1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1592546844474,"user_tz":-480,"elapsed":7224,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"8bfe3e30-24df-4933-8496-5aee32d162b0"},"source":["print('total training horse images:', len(os.listdir(train_horse_dir)))\n","print('total training human images:', len(os.listdir(train_human_dir)))\n","print('total validation horse images:', len(os.listdir(validation_horse_dir)))\n","print('total validation human images:', len(os.listdir(validation_human_dir)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total training horse images: 500\n","total training human images: 527\n","total validation horse images: 128\n","total validation human images: 128\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DDKl6pVmPCnm","colab_type":"code","colab":{}},"source":["#Define the model\n","model = tf.keras.models.Sequential([\n","    # The first convolution\n","    Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n","    MaxPooling2D(2, 2),\n","    # The second convolution\n","    Conv2D(32, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    # The third convolution\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    # The fourth convolution\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    # The fifth convolution\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    # Flatten the results to feed into a DNN\n","    Flatten(),\n","    # 512 neuron hidden layer\n","    Dense(512, activation='relu'),\n","    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n","    Dense(1, activation='sigmoid')   #Signmoid activation because its binary classification\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCRRw0qfPIot","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":635},"executionInfo":{"status":"ok","timestamp":1592546845276,"user_tz":-480,"elapsed":7995,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"73bd1f42-ed8f-46bb-ff58-a62efcd272e6"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 298, 298, 16)      448       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3136)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               1606144   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 513       \n","=================================================================\n","Total params: 1,704,097\n","Trainable params: 1,704,097\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V5CvpbAKPl_n","colab_type":"code","colab":{}},"source":["from tensorflow.keras.optimizers import RMSprop\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(lr=0.001),   #You import RMSprop so you can set learning rate\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbsZOwN8PqPI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592546845279,"user_tz":-480,"elapsed":7957,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"566ddaa7-50e8-4ed6-beb9-3d0cf09f4e50"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255  (Normalize)\n","# Data augmentation occurs here too. Training data is augmented and the augmented images are used for training. The original training images ARE NOT USED for training (only the augmented ones are!)\n","# Only the augmented images are returned. The original is not!\n","# Experiment with the data augmentation parameters. Sometimes, its better without data augmentation to start with (If the test dataset is like the train dataset, of which the train dataset has it's images augmented so the originals (and hence the test dataset) are not trained upon and not recognizable by the model)\n","\n","train_datagen = ImageDataGenerator(rescale=1/255,\n","                                        rotation_range = 40,   #Range of degree to rotate the image by\n","                                        width_shift_range=0.2,  #How much ratio to shift image left and right\n","                                        height_shift_range=0.2, #How much ratio to shift image up and down\n","                                        shear_range=0.2,    #How much to flatten\n","                                        zoom_range=0.2,    #How much to zoom in\n","                                        horizontal_flip=True,   #Flip the image about the y axis\n","                                        fill_mode='nearest')    #Attempt to recreate any missing pixels in the img\n","\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","# Flow training images in batches of 128 using train_datagen generator. The training labels will be auto created for you (Based on the name of the subdirectory 'horse' and 'human')\n","train_generator = train_datagen.flow_from_directory(\n","        '/content/horse-or-human/',  # This is the source directory for training images. Inside this folder contains 2 subdirectories (1 for horse and 1 for human).\n","        target_size=(300, 300),  # All images will be resized to 300x300\n","        batch_size=128,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","        '/content/validation-horse-or-human/',\n","        target_size=(300, 300),\n","        batch_size=32,   #1 epoch = 1 entire pass on the data set = batch_size * steps_per_epoch.    1 step_per_epoch = 1 batch_size amount of pictures processed\n","        class_mode='binary')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1027 images belonging to 2 classes.\n","Found 256 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AoTn1vBFSBE_","colab_type":"code","colab":{}},"source":["class mycallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self,epoch,logs={}):\n","    if(logs.get('accuracy')>0.98):\n","      print('\\n 98% Accuracy reached!')\n","      self.model.stop_training = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlUj1-vRQakL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"status":"ok","timestamp":1592547029968,"user_tz":-480,"elapsed":192610,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"0451d9e4-09b6-4829-e51f-a944b21cd669"},"source":["callbacks = mycallback()\n","\n","model.fit(\n","      train_generator,    #Specify the training generator defined previously. No need for labels as they are created in the train_generator\n","      steps_per_epoch=8,  #Since you have 1027 images, 128*8 = 1024 hence you need 8 steps of batch_size=128 to get 1024 images (And hence count it as 1 epoch)\n","      epochs=15,\n","      verbose=1,\n","      validation_data = validation_generator,\n","      validation_steps=8,  #Since you have 256 images, 32*8 = 256 hence you need 8 steps of batch_size=32 to pass through all 256 images (and hence count it as one epoch)\n","      callbacks=[callbacks])  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","8/8 [==============================] - 14s 2s/step - loss: 0.7869 - accuracy: 0.5250 - val_loss: 0.6641 - val_accuracy: 0.5820\n","Epoch 2/15\n","8/8 [==============================] - 14s 2s/step - loss: 0.8178 - accuracy: 0.6096 - val_loss: 0.6739 - val_accuracy: 0.5781\n","Epoch 3/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.5327 - accuracy: 0.8242 - val_loss: 3.3691 - val_accuracy: 0.5078\n","Epoch 4/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.3158 - accuracy: 0.8732 - val_loss: 1.2874 - val_accuracy: 0.7031\n","Epoch 5/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.3076 - accuracy: 0.8732 - val_loss: 1.3172 - val_accuracy: 0.6289\n","Epoch 6/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.1562 - accuracy: 0.9355 - val_loss: 1.5993 - val_accuracy: 0.6680\n","Epoch 7/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.2058 - accuracy: 0.9209 - val_loss: 1.8418 - val_accuracy: 0.6016\n","Epoch 8/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.0631 - accuracy: 0.9755 - val_loss: 1.7722 - val_accuracy: 0.6836\n","Epoch 9/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.4535 - accuracy: 0.9155 - val_loss: 0.6498 - val_accuracy: 0.6953\n","Epoch 10/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.1993 - accuracy: 0.9266 - val_loss: 1.4845 - val_accuracy: 0.6758\n","Epoch 11/15\n","8/8 [==============================] - 13s 2s/step - loss: 0.1591 - accuracy: 0.9555 - val_loss: 1.5363 - val_accuracy: 0.6680\n","Epoch 12/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9933\n"," 98% Accuracy reached!\n","8/8 [==============================] - 13s 2s/step - loss: 0.0343 - accuracy: 0.9933 - val_loss: 1.8859 - val_accuracy: 0.6914\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5080476898>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ljjSVSbGR4jL","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1592547077929,"user_tz":-480,"elapsed":10589,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"17b9920f-5c1f-430b-92a5-b498f0fe1836"},"source":["import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(300, 300))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a human\")\n","  else:\n","    print(fn + \" is a horse\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-ddc5842c-a071-45de-84d9-ec1b377055fd\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-ddc5842c-a071-45de-84d9-ec1b377055fd\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving human01-17.png to human01-17.png\n","[1.]\n","human01-17.png is a human\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OYE5fNLHTTz8","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}