{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch Binary Classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNML+jVtuYiSa2shO27BslA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Ee9rZviz3LD","executionInfo":{"status":"ok","timestamp":1611919591884,"user_tz":-480,"elapsed":4291,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"4b3e4a57-6301-4b5a-bac3-950596b2c24a"},"source":["import torch\r\n","print(torch.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["1.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0-scmzZk5PVL","executionInfo":{"status":"ok","timestamp":1611920467678,"user_tz":-480,"elapsed":628,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["import numpy as np\r\n","import pandas as pd\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.metrics import accuracy_score\r\n","from torch.utils.data import Dataset, DataLoader, random_split\r\n","from torch import Tensor\r\n","from torch.nn import *\r\n","from torch.optim import SGD\r\n","from torch.nn.init import xavier_uniform_, kaiming_uniform_"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeXE0R248ft0","executionInfo":{"status":"ok","timestamp":1611920897363,"user_tz":-480,"elapsed":806,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["#Define the Dataset\r\n","class CSVDataset(Dataset):\r\n","  def __init__(self, path):\r\n","    df = pd.read_csv(path, header=None)\r\n","    #Get the X and Y array \r\n","    self.X = df.values[:,:-1]\r\n","    self.y = df.values[:,-1]\r\n","    #Ensure that the x and y values are float32\r\n","    self.X = self.X.astype('float32')\r\n","    \r\n","    self.y = LabelEncoder().fit_transform(self.y)\r\n","    self.y = self.y.astype('float32')\r\n","    self.y = self.y.reshape((len(self.y), 1))\r\n","\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.X)\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return [self.X[idx], self.y[idx]]\r\n","\r\n","  def get_splits(self, test_pc=0.33):\r\n","    test_size = round(len(self.X) * test_pc)\r\n","    train_size = len(self.X) - test_size\r\n","    # random_split takes as input the dataset (self) and the length of splits to produce (As specified by train/test_size)\r\n","    # Will return splits for the train and test data. Split into 2, with the given sizes\r\n","    return random_split(self, [train_size, test_size])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDuKZbAK-Meu","executionInfo":{"status":"ok","timestamp":1611923235443,"user_tz":-480,"elapsed":882,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# torch.nn.Module is the base class for all neural network modules. \r\n","# Your models should subclass this class\r\n","class MLP(Module):\r\n","  def __init__(self, n_inputs):\r\n","    super(MLP, self).__init__()\r\n","\r\n","    # Input to first hidden layer\r\n","    # Parameters are the size of the input and outputs to this layer\r\n","    self.hidden1 = Linear(n_inputs, 10)\r\n","    # The weights of the given layer are initialized after the layer is defined\r\n","    kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\r\n","    self.act1 = ReLU()\r\n","\r\n","    # Second hidden layer\r\n","    self.hidden2 = Linear(10, 8)\r\n","    kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\r\n","    self.act2 = ReLU()\r\n","\r\n","    # Third hidden layer\r\n","    self.hidden3 = Linear(8,1)\r\n","    xavier_uniform_(self.hidden3.weight)\r\n","    # Use sigmoid as this is binary classification\r\n","    self.act3 = Sigmoid()\r\n","\r\n","    # Forward Propogation\r\n","  \r\n","  def forward(self, X):\r\n","    # Kinda resembles the Tensorflow Functional Syntax. In fact, its the same....\r\n","    X = self.hidden1(X)\r\n","    X = self.act1(X)\r\n","    X = self.hidden2(X)\r\n","    X = self.act2(X)\r\n","    X = self.hidden3(X)\r\n","    X = self.act3(X)\r\n","    return X"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL13zQNgCNkS","executionInfo":{"status":"ok","timestamp":1611922105292,"user_tz":-480,"elapsed":829,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# Pytorch's DataLoader is an interable over a Dataset\r\n","def prepare_data(path):\r\n","  dataset = CSVDataset(path)\r\n","  train, test = dataset.get_splits()\r\n","  train_dl = DataLoader(train, batch_size=32, shuffle=True)\r\n","  test_dl = DataLoader(test, batch_size=1024, shuffle=False)\r\n","  return train_dl, test_dl"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUSdWwCtC15i","executionInfo":{"status":"ok","timestamp":1611923304664,"user_tz":-480,"elapsed":615,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["def train_model(train_dl, model, epochs):\r\n","  # Define optimizers and loss\r\n","  criterion = BCELoss()\r\n","  # Must pass in the parameters you want to optimize and the additional parameters\r\n","  optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n","\r\n","  step = 0\r\n","\r\n","  # Enumerate number of epochs\r\n","  for epoch in range(epochs):\r\n","    for i, (inputs, targets) in enumerate(train_dl):\r\n","      # Clear the Gradients (I.e set to zero)\r\n","      optimizer.zero_grad()\r\n","\r\n","      # Compute the model output\r\n","      yhat = model(inputs)\r\n","\r\n","      # Calculate loss\r\n","      loss = criterion(yhat, targets)\r\n","\r\n","      # Perform backpropagation to get the gradients\r\n","      # Compute the gradient of the loss with respect to the parameters. This is done for all parameters\r\n","      loss.backward()\r\n","\r\n","      # Update model weights based on the gradients obtained in the previous step\r\n","      optimizer.step()\r\n","\r\n","    print(f'Step {step}/{epochs}...')\r\n","    step += 1"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"nG16ntgpEdhH","executionInfo":{"status":"ok","timestamp":1611923375710,"user_tz":-480,"elapsed":896,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["def evaluate_model(test_dl, model):\r\n","  preds, actuals = [], []\r\n","  for i, (inputs, targets) in enumerate(test_dl):\r\n","    yhat = model(inputs)\r\n","    # Retrieve the numpy array from yhat (Which is currently a pytorch tensor object)\r\n","    yhat = yhat.detach().numpy()\r\n","\r\n","    # Convert the actual (Which is currently a tensor) into numpy and reshape to be same as the yhat shape\r\n","    actual = targets.numpy()\r\n","    actual = actual.reshape((len(actual), 1))\r\n","    # All yhats are currenly a decimal between 0 and 1. Hence, you round to get a whole number\r\n","    yhat = yhat.round()\r\n","\r\n","    preds.append(yhat)\r\n","    actuals.append(actual)\r\n","  # Vstack will vertically combine all the numpy arrays. Works like how extends work for python list. \r\n","  # All but the first dimension of the arrays have to be same\r\n","  preds, actuals = np.vstack(preds), np.vstack(actuals)\r\n","  # Calculate the Accuracy\r\n","  acc = accuracy_score(actuals, preds)\r\n","  return acc"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLcJCgFfF8k8","executionInfo":{"status":"ok","timestamp":1611922967400,"user_tz":-480,"elapsed":1060,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["def predict(data, model):\r\n","  # Convert data to tensor object (To pass in to pytorch)\r\n","  data = Tensor([data])\r\n","  yhat = model(data)\r\n","  #Retrieve np array\r\n","  yhat.detach().numpy()\r\n","  return yhat"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vdyCkKEjGIZ0","executionInfo":{"status":"ok","timestamp":1611923013392,"user_tz":-480,"elapsed":679,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# Prepare the Data\r\n","path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\r\n","train_dl, test_dl = prepare_data(path)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CSzotZtYGLqx","executionInfo":{"status":"ok","timestamp":1611923106231,"user_tz":-480,"elapsed":783,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"08db4a16-e6bc-4343-8130-1d008235820d"},"source":["# Inputs will be a list of inputs [[2,4,5], [3,5,6]]\r\n","# targets will be a list of labels [[0], [1]]\r\n","for i, (inputs, targets) in enumerate(test_dl):\r\n","  print(inputs)\r\n","  # We realize that there are 34 features per data\r\n","  print(inputs.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["tensor([[ 1.0000,  0.0000,  0.5011,  ...,  0.1056,  0.6067, -0.0809],\n","        [ 1.0000,  0.0000,  0.9818,  ...,  0.1646,  0.5884,  0.1708],\n","        [ 1.0000,  0.0000,  0.7152,  ..., -0.1068,  0.3042, -0.0518],\n","        ...,\n","        [ 1.0000,  0.0000,  0.8705,  ..., -0.0594,  0.5146,  0.1664],\n","        [ 0.0000,  0.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n","        [ 1.0000,  0.0000,  0.8063,  ...,  0.2390, -0.2385,  0.3115]])\n","torch.Size([116, 34])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"norW24OMGUlT","executionInfo":{"status":"ok","timestamp":1611923308416,"user_tz":-480,"elapsed":1570,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"fc622c82-a94f-4925-eb1a-1f5e5a6b98c1"},"source":["# Initialize your model with 34 nodes at the first layer (As there are 34 features)\r\n","model = MLP(34)\r\n","train_model(train_dl, model, 100)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Step 0/100...\n","Step 1/100...\n","Step 2/100...\n","Step 3/100...\n","Step 4/100...\n","Step 5/100...\n","Step 6/100...\n","Step 7/100...\n","Step 8/100...\n","Step 9/100...\n","Step 10/100...\n","Step 11/100...\n","Step 12/100...\n","Step 13/100...\n","Step 14/100...\n","Step 15/100...\n","Step 16/100...\n","Step 17/100...\n","Step 18/100...\n","Step 19/100...\n","Step 20/100...\n","Step 21/100...\n","Step 22/100...\n","Step 23/100...\n","Step 24/100...\n","Step 25/100...\n","Step 26/100...\n","Step 27/100...\n","Step 28/100...\n","Step 29/100...\n","Step 30/100...\n","Step 31/100...\n","Step 32/100...\n","Step 33/100...\n","Step 34/100...\n","Step 35/100...\n","Step 36/100...\n","Step 37/100...\n","Step 38/100...\n","Step 39/100...\n","Step 40/100...\n","Step 41/100...\n","Step 42/100...\n","Step 43/100...\n","Step 44/100...\n","Step 45/100...\n","Step 46/100...\n","Step 47/100...\n","Step 48/100...\n","Step 49/100...\n","Step 50/100...\n","Step 51/100...\n","Step 52/100...\n","Step 53/100...\n","Step 54/100...\n","Step 55/100...\n","Step 56/100...\n","Step 57/100...\n","Step 58/100...\n","Step 59/100...\n","Step 60/100...\n","Step 61/100...\n","Step 62/100...\n","Step 63/100...\n","Step 64/100...\n","Step 65/100...\n","Step 66/100...\n","Step 67/100...\n","Step 68/100...\n","Step 69/100...\n","Step 70/100...\n","Step 71/100...\n","Step 72/100...\n","Step 73/100...\n","Step 74/100...\n","Step 75/100...\n","Step 76/100...\n","Step 77/100...\n","Step 78/100...\n","Step 79/100...\n","Step 80/100...\n","Step 81/100...\n","Step 82/100...\n","Step 83/100...\n","Step 84/100...\n","Step 85/100...\n","Step 86/100...\n","Step 87/100...\n","Step 88/100...\n","Step 89/100...\n","Step 90/100...\n","Step 91/100...\n","Step 92/100...\n","Step 93/100...\n","Step 94/100...\n","Step 95/100...\n","Step 96/100...\n","Step 97/100...\n","Step 98/100...\n","Step 99/100...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epCq9VblIXBH","executionInfo":{"status":"ok","timestamp":1611923835630,"user_tz":-480,"elapsed":987,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"1a11d874-457b-407f-8566-a23287f68ff8"},"source":["#Doing this to show how the variables look like\r\n","\r\n","preds, actuals = [], []\r\n","for i, (inputs, targets) in enumerate(test_dl):\r\n","    yhat = model(inputs)\r\n","    # Retrieve the numpy array from yhat (Which is currently a pytorch tensor object)\r\n","    yhat = yhat.detach().numpy()\r\n","\r\n","    # Convert the actual (Which is currently a tensor) into numpy and reshape to be same as the yhat shape\r\n","    actual = targets.numpy()\r\n","    actual = actual.reshape((len(actual), 1))\r\n","    # All yhats are currenly a decimal between 0 and 1. Hence, you round to get a whole number\r\n","    yhat = yhat.round()\r\n","\r\n","    preds.append(yhat)\r\n","    actuals.append(actual)\r\n","preds, actuals = np.vstack(preds), np.vstack(actuals)\r\n","print(preds)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["[[1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [0.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [1.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [1.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vQQMZ4w7G5Ld","executionInfo":{"status":"ok","timestamp":1611923377669,"user_tz":-480,"elapsed":781,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["acc = evaluate_model(test_dl, model)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lu5rO6iIHhKZ","executionInfo":{"status":"ok","timestamp":1611923390026,"user_tz":-480,"elapsed":1333,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"6b82ca5b-f6c3-4fb7-8ecb-75bfa5799a98"},"source":["print(f'Accuracy is {acc}')"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Accuracy is 0.9396551724137931\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62hLFJcbHvXM","executionInfo":{"status":"ok","timestamp":1611923505856,"user_tz":-480,"elapsed":505,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"3e36a830-8054-4e92-dba8-54f3e81ad92a"},"source":["example = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\r\n","yhat = predict(example, model)\r\n","# Must round (To get a number 0 or 1), since yhat will be a decimal between 0 to 1. \r\n","print(f'It predicted: {yhat.round()}')"],"execution_count":34,"outputs":[{"output_type":"stream","text":["It predicted: tensor([[1.]], grad_fn=<RoundBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Ie05-LrIJ1-"},"source":[""],"execution_count":null,"outputs":[]}]}