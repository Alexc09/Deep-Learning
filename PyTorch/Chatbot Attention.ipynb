{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chatbot Attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP6WtIGIectw2d/xlCdwaqW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qomUwoogNPha","executionInfo":{"status":"ok","timestamp":1615446640496,"user_tz":-480,"elapsed":555,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","from __future__ import unicode_literals\r\n","\r\n","import torch\r\n","from torch.jit import script, trace\r\n","import torch.nn as nn\r\n","from torch import optim\r\n","import torch.nn.functional as F\r\n","import csv\r\n","import random\r\n","import re\r\n","import os\r\n","import unicodedata\r\n","import codecs\r\n","from io import open\r\n","import itertools\r\n","import math\r\n","import zipfile\r\n","import pandas as pd\r\n","\r\n","\r\n","USE_CUDA = torch.cuda.is_available()\r\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bw5kUHEK84YA","executionInfo":{"status":"ok","timestamp":1615444889285,"user_tz":-480,"elapsed":1518,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"ef725a95-76c0-4d0d-c551-20c80244b47a"},"source":["!wget https://dl.dropboxusercontent.com/s/2n5lwz8igwquo1h/movie.zip?dl=0 -O /content/data.zip"],"execution_count":10,"outputs":[{"output_type":"stream","text":["--2021-03-11 06:41:27--  https://dl.dropboxusercontent.com/s/2n5lwz8igwquo1h/movie.zip?dl=0\n","Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n","Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9247312 (8.8M) [application/zip]\n","Saving to: ‘/content/data.zip’\n","\n","/content/data.zip   100%[===================>]   8.82M  52.1MB/s    in 0.2s    \n","\n","2021-03-11 06:41:28 (52.1 MB/s) - ‘/content/data.zip’ saved [9247312/9247312]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n9tOSwlH_Wpx","executionInfo":{"status":"ok","timestamp":1615444889545,"user_tz":-480,"elapsed":1520,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["train_zip = '/content/data.zip'\r\n","train_dir = '/content/data'\r\n","\r\n","\r\n","with zipfile.ZipFile(train_zip, 'r') as zip_ref:\r\n","    zip_ref.extractall(train_dir)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXJF_fM-Ip4s","executionInfo":{"status":"ok","timestamp":1615446846317,"user_tz":-480,"elapsed":765,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["def printLines(file, n=10):\r\n","    with open(file, 'rb') as datafile:\r\n","        lines = datafile.readlines()\r\n","    for line in lines[:n]:\r\n","        print(line)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZsLznm4YBJ8S","executionInfo":{"status":"ok","timestamp":1615444984401,"user_tz":-480,"elapsed":741,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"38211ca5-9bd3-4c29-f5e2-65057c5039d6"},"source":["with open('/content/data/movie_lines.txt', 'rb') as corpus:\r\n","  lines = corpus.readlines()\r\n","  for line in lines[:10]:\r\n","    print(line)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n","b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n","b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n","b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n","b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n","b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n","b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n","b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n","b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n","b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AYspghwABj8k","executionInfo":{"status":"ok","timestamp":1615445301550,"user_tz":-480,"elapsed":734,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# Split each line into a dictionary of fields (lineID, characterID, movieID, character, text)\r\n","def loadLines(path, fields):\r\n","  lines = {}\r\n","  with open(path, 'r', encoding='iso-8859-1') as f:\r\n","    for line in f:\r\n","      values = line.split(' +++$+++ ')\r\n","      lineObj = {}\r\n","      for i, field in enumerate(fields):\r\n","        lineObj[field] = values[i]\r\n","      lines[lineObj['lineID']] = lineObj\r\n","  # returns a dictionary containing the dictionaries representing each line\r\n","  # {'L871': {'lineID': ...}, 'L987': {}}\r\n","  return lines"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxPFHVkgCxYG","executionInfo":{"status":"ok","timestamp":1615446452436,"user_tz":-480,"elapsed":543,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# movie_conversations.txt groups lines into conversations, using the ID of these lines \r\n","# I.e (Conversation 1: [L453, L465, L436])\r\n","# Group lines into their respective conversations\r\n","def loadConversations(path, lines, fields):\r\n","  conversations = []\r\n","  with open(path, 'r', encoding='iso-8859-1') as f:\r\n","    for line in f:\r\n","      values = line.split(' +++$+++ ')\r\n","      convObj = {}\r\n","      for i, field in enumerate(fields):\r\n","        convObj[field] = values[i]\r\n","      # Use this pattern to find all strings that contain this pattern of 'L<Numbers>'\r\n","      utterance_id_pattern = re.compile('L[0-9]+')\r\n","      lineIds = utterance_id_pattern.findall(convObj['utteranceIDs'])\r\n","      convObj['lines'] = []\r\n","      for lineId in lineIds:\r\n","        convObj['lines'].append(lines[lineId])\r\n","      conversations.append(convObj)\r\n","  return conversations"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BlMbETxEtY1","executionInfo":{"status":"ok","timestamp":1615446453854,"user_tz":-480,"elapsed":795,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# Create q_a pairs using the conversation list. \r\n","def extractSentencePairs(conversations):\r\n","  qa_pairs = []\r\n","  for conversation in conversations:\r\n","    for i in range(len(conversation['lines']) - 1):\r\n","      inputLine = conversation['lines'][i]['text'].strip()\r\n","      targetLine = conversation['lines'][i+1]['text'].strip()\r\n","      # Ensures that the lines are not empty\r\n","      if inputLine and targetLine:\r\n","        qa_pairs.append([inputLine, targetLine])\r\n","    \r\n","  return qa_pairs"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpfGvJi1FSpi","executionInfo":{"status":"ok","timestamp":1615446455056,"user_tz":-480,"elapsed":1863,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["dpath = '/content/data/'\r\n","lines = {}\r\n","conversations = {}\r\n","MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\r\n","MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\r\n","\r\n","lines = loadLines(dpath + 'movie_lines.txt', MOVIE_LINES_FIELDS)\r\n","conversations = loadConversations(dpath+'movie_conversations.txt', lines, MOVIE_CONVERSATIONS_FIELDS)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Fig-vQ-HE7F","executionInfo":{"status":"ok","timestamp":1615446825619,"user_tz":-480,"elapsed":1956,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["delimiter = '\\t'\r\n","delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\r\n","\r\n","with open(dpath + 'formatted_data.txt', 'w', encoding='utf-8') as f:\r\n","  writer = csv.writer(f, delimiter=delimiter, lineterminator='\\n')\r\n","  for pair in extractSentencePairs(conversations):\r\n","    writer.writerow(pair)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_wFEDjUHnyx","executionInfo":{"status":"ok","timestamp":1615446859775,"user_tz":-480,"elapsed":1968,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"37d6936d-3a98-4fa0-b2e7-2158bdcb45c5"},"source":["printLines(dpath+'formatted_data.txt')"],"execution_count":32,"outputs":[{"output_type":"stream","text":["b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n","b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n","b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n","b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n","b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n","b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n","b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n","b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n","b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n","b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1rYO-LT_H8jp","executionInfo":{"status":"ok","timestamp":1615448795919,"user_tz":-480,"elapsed":918,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["PAD_token = 0\r\n","SOS_token = 1\r\n","EOS_token = 2\r\n","\r\n","class Voc:\r\n","  def __init__(self, name):\r\n","    self.name = name\r\n","    self.trimmed = False\r\n","    self.word2index = {}\r\n","    self.word2count = {}\r\n","    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\r\n","    self.num_words = 3\r\n","\r\n","  def addSentence(self, sentence):\r\n","    for word in sentence.split(' '):\r\n","      self.addWord(word)\r\n","  \r\n","  def addWord(self, word):\r\n","    if word not in self.word2index:\r\n","      self.word2index[word] = self.num_words\r\n","      self.word2count[word] = 1\r\n","      self.index2word[self.num_words] = word\r\n","      self.num_words += 1\r\n","    else:\r\n","      self.word2count[word] += 1\r\n","  \r\n","  def trim(self, min_count):\r\n","    if self.trimmed:\r\n","      return\r\n","    self.trimmed = True\r\n","    keep_words = []\r\n","\r\n","    for word, occurance in self.word2count.items():\r\n","      if occurance >= min_count:\r\n","        keep_words.append(word)\r\n","    \r\n","    print(f'Keep_words {len(keep_words)}, {len(keep_words)/len(self.word2index)}%')\r\n","\r\n","    # Reset all the parameters. Will have to do addWord again to populate the dictionaries\r\n","    self.word2index = []\r\n","    self.word2count = {}\r\n","    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\r\n","    self.num_words = 3\r\n","\r\n","    for word in keep_words:\r\n","      self.addWord(word)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZyzm3HuKMDb","executionInfo":{"status":"ok","timestamp":1615449276893,"user_tz":-480,"elapsed":13689,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"4d2b4f01-f0ed-4ff4-a410-30167e959102"},"source":["# Maximum sentence length to consider. Any longer sentences will not be considered\r\n","MAX_LENGTH = 10\r\n","\r\n","def unicodetoAscii(s):\r\n","  return ''.join(\r\n","    c for c in unicodedata.normalize('NFD', s)\r\n","    if unicodedata.category(c) != 'Mn'\r\n","  )\r\n","\r\n","# Lowercase, trim and remove non-letter characters\r\n","def normalizeString(s):\r\n","  s = unicodetoAscii(s.lower().strip())\r\n","  s = re.sub(r\"([.!?])\", r\" \\1\", s)\r\n","  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\r\n","  s = re.sub(r\"\\s+\", r\" \", s).strip()\r\n","  return s\r\n","\r\n","\r\n","def readVocs(datafile, corpus_name):\r\n","  lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\r\n","  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\r\n","  voc = Voc(corpus_name)\r\n","  return voc, pairs\r\n","\r\n","\r\n","# Returns True if both sentences in a pair are under the MAX_LENGTH threshold\r\n","def filterPair(p):\r\n","    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\r\n","\r\n","\r\n","def filterPairs(pairs):\r\n","  return [pair for pair in pairs if filterPair(pair)]\r\n","\r\n","\r\n","def loadPrepareData(corpus, corpus_name, datafile, save_dir):\r\n","  voc, pairs = readVocs(datafile, corpus_name)\r\n","  print(f'Read {len(pairs)} sentence pairs')\r\n","  pairs = filterPairs(pairs)\r\n","  print(f'Trimmed to {len(pairs)} sentence pairs')\r\n","  for pair in pairs:\r\n","    voc.addSentence(pair[0])\r\n","    voc.addSentence(pair[1])\r\n","  print(f'{voc.num_words} in total')\r\n","  return voc, pairs\r\n","\r\n","\r\n","save_dir = '/content/data/save/'\r\n","corpus_name = 'movie'\r\n","voc, pairs = loadPrepareData(corpus, corpus_name, dpath + 'formatted_data.txt', save_dir)\r\n","for pair in pairs[:10]:\r\n","  print(pair)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Read 221282 sentence pairs\n","Trimmed to 64271 sentence pairs\n","18008 in total\n","['there .', 'where ?']\n","['you have my word . as a gentleman', 'you re sweet .']\n","['hi .', 'looks like things worked out tonight huh ?']\n","['you know chastity ?', 'i believe we share an art instructor']\n","['have fun tonight ?', 'tons']\n","['well no . . .', 'then that s all you had to say .']\n","['then that s all you had to say .', 'but']\n","['but', 'you always been this selfish ?']\n","['do you listen to this crap ?', 'what crap ?']\n","['what good stuff ?', 'the real you .']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"qJAbgXACOSRX","executionInfo":{"status":"error","timestamp":1615449278767,"user_tz":-480,"elapsed":639,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}},"outputId":"e1d998e9-7666-4372-c4df-23480f87f8fa"},"source":["MIN_COUNT = 3\r\n","\r\n","def trimRareWords(voc, pairs, MIN_COUNT):\r\n","  # Remove all words whose occurance are below the min count\r\n","  voc.trim(MIN_COUNT)\r\n","  keep_pairs = []\r\n","  for pair in pairs:\r\n","    input_sentence = pair[0]\r\n","    output_sentence = pair[1]\r\n","    keep_input = True\r\n","    keep_output = True\r\n","    for word in input_sentence.split(' '):\r\n","      if word not in voc.word2index:\r\n","        keep_input = False\r\n","        break\r\n","      \r\n","    for word in output_sentence.split(' '):\r\n","      if word not in voc.word2index:\r\n","        keep_output = False\r\n","        break\r\n","    \r\n","    if keep_input and keep_output:\r\n","      # Only append if both sentences are accepted\r\n","      keep_pairs.append(pair)\r\n","\r\n","  \r\n","  print(f'Trimmed from {len(pairs)} pairs to {len(keep_pairs)} pairs')\r\n","\r\n","  return keep_pairs\r\n","\r\n","\r\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Keep_words 7823, 0.43449041932796445%\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-d1a9f1023ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimRareWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-52-d1a9f1023ba7>\u001b[0m in \u001b[0;36mtrimRareWords\u001b[0;34m(voc, pairs, MIN_COUNT)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrimRareWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Remove all words whose occurance are below the min count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMIN_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mkeep_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-5713146b6d2c>\u001b[0m in \u001b[0;36mtrim\u001b[0;34m(self, min_count)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeep_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-5713146b6d2c>\u001b[0m in \u001b[0;36maddWord\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maddWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"]}]},{"cell_type":"code","metadata":{"id":"PjmPAAgkQYto"},"source":[""],"execution_count":null,"outputs":[]}]}