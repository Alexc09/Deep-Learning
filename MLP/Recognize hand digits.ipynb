{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recognize hand digits.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDenGvs77np7g8IJflXdSh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"N5F6XhAezK7S","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJL-gYy4zO1Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"1aad14ac-a36c-4e1e-fc25-14ae9be11cd9","executionInfo":{"status":"ok","timestamp":1590412903520,"user_tz":-480,"elapsed":1432,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["mnist = keras.datasets.mnist    #Load the mnist handwritten digits database\n","\n","\n","#Format the data\n","\n","(x_train,y_train), (x_test,y_test) = mnist.load_data()     #load the train and test sets\n","x_train = x_train.reshape(60000, -1)   #Initially, X_train has dims (m,28,28) where m is the no of training sets and 28 is the height and weights. You are flattening the pic so you can feed into dense layer. set as '-1' to automatically set it to the shape such that the total no of pixels are unchanged\n","x_test = x_test.reshape(10000, -1)   \n","x_train = x_train.astype('float32')   #When you train, its common to use float32 datatype \n","x_test = x_test.astype('float32')\n","\n","x_train /= 255  #Normalize inputs, as the pixels have a max value of 255. \n","x_test /= 255 \n","\n","print(f'There are {x_train.shape[0]} training examples')\n","print(f'There are {x_test.shape[0]} test examples')\n","\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)  #One hot encode your train and test labels, using 10 classes (because you have 10 different types of digits)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n","\n","\n","\n","# Build the model. Use a Dense layer and Dropout layer\n","\n","model = tf.keras.models.Sequential()\n","model.add(keras.layers.Dense(128,input_shape=(784,) ,activation='relu',name = 'dense_layer_1'))   #The 1st and 3nd layers have 128 input nodes. Remember to specify the input shape for the first layer (ignore the no. of train examples. Only care about the other dimension '784' which is basically flattened image)\n","model.add(keras.layers.Dropout(0.3))  #The 2nd and 4th layers are dropout layers, only have to specify the probability of dropout. Serves as regularization (Prevent overfitting)\n","model.add(keras.layers.Dense(128,activation='relu',name = 'dense_layer_2'))\n","model.add(keras.layers.Dropout(0.3))\n","model.add(keras.layers.Dense(10,activation='softmax',name = 'dense_layer_final'))  #This is the final layer, so it has 10 nodes (since it predicts out of 10 classes). Use a softmax activation to give a final value\n","\n","model.summary()  #Summary of the model\n","\n","\n","\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["There are 60000 training examples\n","There are 10000 test examples\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_layer_1 (Dense)        (None, 128)               100480    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_layer_2 (Dense)        (None, 128)               16512     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_layer_final (Dense)    (None, 10)                1290      \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h-uRzGfzzd1e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3867748b-8c4d-4def-eff3-b304f49f9530","executionInfo":{"status":"ok","timestamp":1590412982626,"user_tz":-480,"elapsed":76250,"user":{"displayName":"Alex C","photoUrl":"","userId":"12397117775642959911"}}},"source":["# Compile the model. Specify what optimizers,loss and metrics to use\n","model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","\n","# Train the model\n","model.fit(x_train,y_train,batch_size=128, epochs=50, verbose=1, validation_split=0.2) #Keep 0.2 of the train set as my validation set\n","\n","\n","# Evaluate the model\n","test_loss , test_acc = model.evaluate(x_test, y_test)  #Evaluate using the test set\n","print(f'Test accuracy: {test_acc}')\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","375/375 [==============================] - 2s 4ms/step - loss: 1.7045 - accuracy: 0.4649 - val_loss: 0.9028 - val_accuracy: 0.8077\n","Epoch 2/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.9232 - accuracy: 0.7135 - val_loss: 0.5381 - val_accuracy: 0.8669\n","Epoch 3/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.7878 - val_loss: 0.4259 - val_accuracy: 0.8894\n","Epoch 4/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.5909 - accuracy: 0.8201 - val_loss: 0.3709 - val_accuracy: 0.8997\n","Epoch 5/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.5291 - accuracy: 0.8420 - val_loss: 0.3373 - val_accuracy: 0.9069\n","Epoch 6/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4877 - accuracy: 0.8546 - val_loss: 0.3144 - val_accuracy: 0.9107\n","Epoch 7/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4559 - accuracy: 0.8658 - val_loss: 0.2964 - val_accuracy: 0.9137\n","Epoch 8/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4311 - accuracy: 0.8744 - val_loss: 0.2804 - val_accuracy: 0.9193\n","Epoch 9/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4078 - accuracy: 0.8797 - val_loss: 0.2683 - val_accuracy: 0.9222\n","Epoch 10/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3904 - accuracy: 0.8839 - val_loss: 0.2575 - val_accuracy: 0.9252\n","Epoch 11/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3774 - accuracy: 0.8901 - val_loss: 0.2476 - val_accuracy: 0.9287\n","Epoch 12/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3586 - accuracy: 0.8931 - val_loss: 0.2401 - val_accuracy: 0.9302\n","Epoch 13/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3483 - accuracy: 0.8963 - val_loss: 0.2314 - val_accuracy: 0.9333\n","Epoch 14/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3360 - accuracy: 0.9012 - val_loss: 0.2253 - val_accuracy: 0.9348\n","Epoch 15/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3240 - accuracy: 0.9047 - val_loss: 0.2182 - val_accuracy: 0.9368\n","Epoch 16/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3171 - accuracy: 0.9070 - val_loss: 0.2126 - val_accuracy: 0.9377\n","Epoch 17/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3059 - accuracy: 0.9090 - val_loss: 0.2066 - val_accuracy: 0.9399\n","Epoch 18/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2999 - accuracy: 0.9121 - val_loss: 0.2022 - val_accuracy: 0.9421\n","Epoch 19/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2920 - accuracy: 0.9141 - val_loss: 0.1980 - val_accuracy: 0.9432\n","Epoch 20/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2810 - accuracy: 0.9172 - val_loss: 0.1931 - val_accuracy: 0.9443\n","Epoch 21/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2770 - accuracy: 0.9181 - val_loss: 0.1886 - val_accuracy: 0.9452\n","Epoch 22/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.9198 - val_loss: 0.1847 - val_accuracy: 0.9463\n","Epoch 23/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2705 - accuracy: 0.9209 - val_loss: 0.1811 - val_accuracy: 0.9478\n","Epoch 24/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2584 - accuracy: 0.9243 - val_loss: 0.1785 - val_accuracy: 0.9485\n","Epoch 25/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2580 - accuracy: 0.9243 - val_loss: 0.1742 - val_accuracy: 0.9495\n","Epoch 26/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2541 - accuracy: 0.9262 - val_loss: 0.1724 - val_accuracy: 0.9496\n","Epoch 27/50\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2478 - accuracy: 0.9275 - val_loss: 0.1691 - val_accuracy: 0.9507\n","Epoch 28/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2447 - accuracy: 0.9295 - val_loss: 0.1664 - val_accuracy: 0.9515\n","Epoch 29/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2388 - accuracy: 0.9302 - val_loss: 0.1641 - val_accuracy: 0.9527\n","Epoch 30/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2357 - accuracy: 0.9308 - val_loss: 0.1613 - val_accuracy: 0.9530\n","Epoch 31/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2330 - accuracy: 0.9323 - val_loss: 0.1597 - val_accuracy: 0.9536\n","Epoch 32/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2284 - accuracy: 0.9329 - val_loss: 0.1570 - val_accuracy: 0.9539\n","Epoch 33/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2241 - accuracy: 0.9342 - val_loss: 0.1550 - val_accuracy: 0.9543\n","Epoch 34/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2209 - accuracy: 0.9365 - val_loss: 0.1535 - val_accuracy: 0.9549\n","Epoch 35/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2185 - accuracy: 0.9354 - val_loss: 0.1513 - val_accuracy: 0.9559\n","Epoch 36/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2138 - accuracy: 0.9373 - val_loss: 0.1490 - val_accuracy: 0.9570\n","Epoch 37/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2118 - accuracy: 0.9382 - val_loss: 0.1473 - val_accuracy: 0.9572\n","Epoch 38/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2088 - accuracy: 0.9391 - val_loss: 0.1458 - val_accuracy: 0.9570\n","Epoch 39/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2053 - accuracy: 0.9396 - val_loss: 0.1439 - val_accuracy: 0.9576\n","Epoch 40/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2014 - accuracy: 0.9406 - val_loss: 0.1425 - val_accuracy: 0.9578\n","Epoch 41/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2008 - accuracy: 0.9401 - val_loss: 0.1423 - val_accuracy: 0.9569\n","Epoch 42/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2004 - accuracy: 0.9414 - val_loss: 0.1391 - val_accuracy: 0.9587\n","Epoch 43/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1970 - accuracy: 0.9423 - val_loss: 0.1380 - val_accuracy: 0.9598\n","Epoch 44/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1932 - accuracy: 0.9431 - val_loss: 0.1364 - val_accuracy: 0.9595\n","Epoch 45/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1880 - accuracy: 0.9452 - val_loss: 0.1355 - val_accuracy: 0.9593\n","Epoch 46/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1880 - accuracy: 0.9448 - val_loss: 0.1341 - val_accuracy: 0.9598\n","Epoch 47/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1881 - accuracy: 0.9444 - val_loss: 0.1329 - val_accuracy: 0.9607\n","Epoch 48/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1838 - accuracy: 0.9458 - val_loss: 0.1309 - val_accuracy: 0.9614\n","Epoch 49/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1784 - accuracy: 0.9471 - val_loss: 0.1311 - val_accuracy: 0.9611\n","Epoch 50/50\n","375/375 [==============================] - 1s 4ms/step - loss: 0.1799 - accuracy: 0.9471 - val_loss: 0.1290 - val_accuracy: 0.9617\n","313/313 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9630\n","Test accuracy: 0.9629999995231628\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VaxIXHfe_w8n","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}